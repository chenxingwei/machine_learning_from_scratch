# Logistic Regression

We have previously go over linear regression with mathematical methods and gradient regression. Here we will implement logistic regression
using gradient descent.

In linear regression, we have independent variables and dependent variable, and the dependent variable is continuous variable. Everything is the same for logistic regression but the dependent variable is not a continuous variable. The dependent variable in logistic regression is a binary variable, for example, whether is a female or male, something like this. So the dependent variable is always expressed as:

![equation](http://latex.codecogs.com/gif.latex?y=[0,1,1,0,...,1,0,1])

We used to set the binary variable to 0 and 1. Of course we can solve this problem with linear regression directly, as input and output of logistic regression also satisfy the requirement for linear regression. But the results are always bad. 

## Sigmoid function

We first give the function of sigmoid function and curves as shown blow:

![equation](http://latex.codecogs.com/gif.latex?h(x)=\frac{1}{1+e^{-x}})

and its curve:

![](https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/images/lr3_001.png)

The sigmoid function have defination for every real number, and its range of the dependent variable is from 0 to 1. Taking the category 1 as standard, sigmoid function can be thought to be the probability the input sample is belong to the category 1. Notably, when sigmoid function returns value larger than 0.5, meaning the sample has a probability larger than 0.5 to be in categories 1, so we can category it into the 1 class. Otherwise, with sigmoid function output less than 0.5, category it into the 0 class.

Advantages to use logistic regression:

1. Sigmoid function is derivable and it is very easy to get its derivatives.
2. Probability explaination of sigmoid function gives understanding of the function.

## Cross entropy loss function

We can't use ordinary least square (OLS) loss function and the mean absolute error (MAE) loss function. It we use these two loss functions, it goes back to linear regression.

The loss function we use the called the cross entropy loss function, here will include some simple knowledge from information theory. I will very briefly introduce the basis of the cross entropy:

1. Firstly we need to know  what is information (in information theory). 

Many things happen randomly, but the things happen with different information. If one thing here happens with probability ![equation](http://latex.codecogs.com/gif.latex?p), then the information of it can be defined as ![equation](http://latex.codecogs.com/gif.latex?-log(p)). 

That is things happen with low probability takes more information. This can be explained or understand through a real world example: We define two things, A is a driver overspeeds and B is car accident caused 10 people die and 20 people injury. Human can easy find B is much more serious than A. A journalist will also report B instead of A because B takes more information. In dayly life, overspeed happens over again and again, but car accident with so many people die and injury rarely happens, the probability for overspeed is much higher than the car accident. Based on the defination of information, B takes more information than A. That is why B will be reported by journalist and B will be known to much more people.

2. Secondly we need to know what is entropy.

Entropy can be understand as the average information of a distribution. For example, we toss the coin, having head and tail with probability 0.5 each. Then the entropy for toss a coin is calculated as 

![equation](http://latex.codecogs.com/gif.latex?entropy=-0.5log(0.5)-0.5log(0.5))

Two two terms in the formula represent toss a head and a tail respectively, more generally, the entropy is defined as:

![equation](http://latex.codecogs.com/gif.latex?entropy=E[-log(p)]=-\sum_{i=1}^{n}p_ilog{p_i})

After then, we comes to the cross entropy, we have real dependent variable (0 or 1), which is also called labels: ![equation](http://latex.codecogs.com/gif.latex?y=[y_1,y_2,...,y_n]). And at the same time, we have the model predicted labels: ![equation](http://latex.codecogs.com/gif.latex?a=[a_1,a_2,...,a_n]), which range from 0 to 1, indicating the probability to be categoried into the 1 class. 

Take out one of them for example, that we have real label ![equation](http://latex.codecogs.com/gif.latex?y_i) and predicted probability ![equation](http://latex.codecogs.com/gif.latex?a_i), this means we have ![equation](http://latex.codecogs.com/gif.latex?a_i) probability to category the sample into 1 class, and we also have ![equation](http://latex.codecogs.com/gif.latex?1-a_i) probability to category the sample into 0 class. Similar as the entropy, the cross entropy multiple the real label as shown below:

![equation](http://latex.codecogs.com/gif.latex?J_i=-y_ilog(a_i)-(1-y_i)log(1-a_i))

Finally, the cross entropy is defined as:

![equation](http://latex.codecogs.com/gif.latex?J=-\sum_{i=1}^{n}(y_ilog(a_i)+(1-y_i)log(1-a_i)))

## Logistic regression

Then is the detail process of logistic regression, similarly as linear regression, we have independent variables and dependent variables. 

![equation](http://latex.codecogs.com/gif.latex?X=[x_1,x_2,...,x_n]^T=\begin{bmatrix}x_{11}&x_{12}&...&x_{1m}\\\x_{21}&x_{22}&...&x_{2m}\\\\...&...&...&...\\\\x_{n1}&x_{n2}&...&x_{nm}\end{bmatrix})

Here ![equation](http://latex.codecogs.com/gif.latex?x_i) is a row vectors expressed as:

![equation](http://latex.codecogs.com/gif.latex?x_i=[x_{i1},x_{i2},...,x_{im}])

And the depedent value is:

![equation](http://latex.codecogs.com/gif.latex?y=[y_1,y_2,...,y_n]^T,y_i\in\{0,1\})

In logistic linear regression, we use sigmoid function to handle the relationship between independent variable and dependent variable. 

We can then combine the weight and bias parameter into new parameter as:

![equation](http://latex.codecogs.com/gif.latex?\theta=[w_1,w_2,...,w_m,b]^T)

In order cooperate with the parameter, we update the sample vector as:

![equation](http://latex.codecogs.com/gif.latex?x_i=[x_{i1},x_{i2},...,x_{im},1])

Then finally we represent the predicted class probability using ![equation](http://latex.codecogs.com/gif.latex?a_i) for the ith sample with following sigmoid function:

![equation](http://latex.codecogs.com/gif.latex?a_i=\frac{1}{1+e^{-x_i\theta}}=\frac{1}{1+e^{-\sum_{k=1}^{m+1}x_{ik}\theta_k}})

We have state to use the cross entropy loss function in logistic regression for only one sample as:

![equation](http://latex.codecogs.com/gif.latex?J(x_i)=-y_ilog(a_i)-(1-y_i)log(1-a_i))

And similar to add up across all samples:

![equation](http://latex.codecogs.com/gif.latex?J=-\sum_{i=1}^{n}(y_ilog(a_i)+(1-y_i)log(1-a_i)))

## Gradient descent for logistic regression

We calculate the the gradient of the loss function using the chain rule.

First is the gradient for one sample:

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J(x_i)}}{\partial{a_i}}=-\frac{y_i}{a_i}-\frac{1-y_i}{1-a_i}=-\frac{y_i-a_iy_i}{a_i(1-a_i)}+\frac{a_i-a_iy_i}{a_i(1-a_i)}=\frac{a_i-y_i}{a_i(1-a_i)})

We can set a temporary variable 

![equation](http://latex.codecogs.com/gif.latex?f_i=-x_i\theta=-\sum_{k=1}^{m+1}x_{ik}\theta_{k})

Then

![equation](http://latex.codecogs.com/gif.latex?a_i=\frac{1}{1+e^{f_i}})

So we get

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{a_i}}{\partial{f_i}}=\frac{e^{f_i}}{(1+e^{f_i})^2}=\frac{1}{1+e^{f_i}}\frac{1+e^{f_i}-1}{1+e^{f_i}}=\frac{1}{1+e^{f_i}}(1-\frac{1}{1+e^{f_i}})=a_i(1-a_i))

and 

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{f_i}}{\partial{\theta_j}}=-x_{ij})

Based on the chain rule, we can have:

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J(x_i)}}{\partial{\theta_j}}=\frac{\partial{J(x_i)}}{\partial{a_i}}\frac{\partial{a_i}}{\partial{f_i}}\frac{\partial{J(f_i)}}{\partial{\theta_j}}=\frac{a_i-y_i}{a_i(1-a_i)}a_i(1-a_i)(-x_{ij})=-x_{ij}(a_i-y_i))

Then we vectorize it across all parameters:

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J(x_i)}}{\partial{\theta}}=-x_{i}(a_i-y_i))

And it is also to vectorize it across all samples.

## Python solution from scratch for logistic regression

With the gradient descent algorithms, logistic regression is also quite easy to get the results.

```python
import os, glob, math
import numpy as np
import random

class logistic_regression_GD:
  """
  logistic regression with crossentropy loss function, and solving by gradient descent methods.
  """
  def __init__(self):
    self.w = None
    self.b = None
    self.y_pred = None
  
  def train(self, X, y, learning_rate=0.001, epoch=30000, tol=1e-4, methods="crossentropy"):
    """
    If data not the required format, this function not works
    @param X, independent variable, example: X = np.array([[1,2,],[1,3,],[1,4], ...])
    @param y, dependent variable, example y = np.array([[1],[2],[3],[4],...])
    Both X and y should be an array.
    @param learning_rate, step size for gradient descent
    @param epoch, times the training process go over the whole datasets
    @param tol, the stop critera, when loss function did not optimize more than tol for 10 times, stop training
    @param methods, the loss function, OLS stands for ordinary least square loss function
    @return, this function will return weight vector w and bias parameter b
    """
    n = len(X)
    self.n = n
    m = len(X[0])
    if len(y) != n:
      print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
    
    # following codes reformat the input
    X = np.array(X)
    y = np.array(y)
    
    if len(y.shape) != 1:
      y = y.reshape(n)
    
    # add a 1 column for the bias variable
    X = np.column_stack((X, np.ones(n)))
    self.X = X
    self.y = y
    
    # get the parameter w and b, first initize
    theta = np.random.randn(m+1)
    num = 0
    for i in xrange(epoch):
      theta -= learning_rate * self.getGradient(theta, methods)
      self.epoch = i
    self.w = theta[:-1]
    self.b = theta[-1]
    return theta
  
  def getGradient(self, theta, methods="crossentropy"):
    """
    Get the gradient of the loss function using parameter theta
    """
    methods_from = ["crossentropy"]
    if methods == "crossentropy":
      a = 1.0 / (1+np.exp(-1.0*(self.X.dot(theta))))- self.y
      a = np.reshape(self.n,1)
      delta = np.sum(self.X*a, axis=0)
    else:
      print "Error, the methods parameter can only be ", methods_from
    return delta
  
  def predict(self, X, y=None):
    """
    @param X, independent variable.
    Example: X = np.array([[1,2,],[1,3,],[1,4], ...])
    @param y, dependent variable, example y = np.array([[1],[2],[3],[4],...])
    @return, return the predicted dependent variables for input X, or if y is specified
    """
    n = len(X)
    X = np.array(X)
    y_pred = X.dot(self.w) + self.b
    self.y_pred = y_pred
    if y != None:
      if n != len(y):
        print "Error, the input X, y should be the same length, while you have len(X)=%d and len(y)=%d"%(n, len(y))
      y = np.array(y)
      y_pred = y_pred.reshape(n)
      if len(y.shape) != 1:
        y = y.reshape(n)
      self.rmse = np.sqrt(np.sum(np.square(y-y_pred))) / n
    return self.y_pred


```

